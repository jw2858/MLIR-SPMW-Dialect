[
  {
    "role": "user",
    "content": "- cornell-zhang.github.io/\n  - allo/\n    - dive/\n      - dataflow.html/\n        - index.md"
  },
  {
    "role": "user",
    "content": "cornell-zhang.github.io/allo/dive/dataflow.html/index.md"
  },
  {
    "role": "user",
    "content": "Dataflow Programming \u2014 Allo Documentation\n\n[|||](# \"Toggle sidebar\")\n\n[Allo Documentation](../index.html \"Go to homepage\")\n====================================================\n\n\n\n\n### Quick search\n\nGetting Started\n\n* [Installation](../setup/index.html)\n\nTutorials\n\n* [Getting Started](../gallery/tutorial_01_get_started.html)\n* [Vivado/Vitis HLS Backend](../gallery/tutorial_02_vhls.html)\n\nDeep Dive\n\n* [Frontend Syntax Guide](frontend_syntax.html)\n* [Dataflow Programming](#)\n* [Data Types and Type Casting](../gallery/dive_01_data_types.html)\n* [Template Kernels](../gallery/dive_02_template.html)\n* [Kernel Composition](../gallery/dive_03_composition.html)\n* [IP Integration](ip.html)\n* [PyTorch Integration](pytorch.html)\n* [Equivalence Checking](../gallery/dive_05_verifier.html)\n* [Other Features](../gallery/dive_04_features.html)\n\nBackends\n\n* [LLVM (CPU)](../backends/llvm.html)\n* [AMD Vitis HLS (FPGA)](../backends/vitis.html)\n* [Siemens Catapult HLS (FPGA)](../backends/catapult.html)\n* [RapidStream TAPA (FPGA)](../backends/tapa.html)\n* [Google XLS (ASIC)](../backends/xls.html)\n* [Multi-Threaded Simulator (CPU)](../backends/simulator.html)\n* [AMD MLIR-AIE (AI Engine)](../backends/aie/index.html)\n\nDeveloper Guide\n\n* [Developer Setup](../developer/index.html)\n* [IR Builder Walkthrough](../gallery/developer_01_ir_builder.html)\n* [MLIR Translation Guide](../gallery/developer_02_mlir.html)\n\nPython API\n\n* [Schedule Primitives](../api/index.html)\n* [Data Types](../api/index.html#data-types)\n\nDataflow Programming[\u00b6](#dataflow-programming \"Link to this heading\")\n=====================================================================\n\nThis document describes the dataflow programming model in Allo, which enables\nspatial parallelism through explicit kernel decomposition and stream-based\ncommunication.\n\nTable of Contents\n\n* [Overview](#overview)\n\n  + [Key Concepts](#key-concepts)\n* [Getting Started](#getting-started)\n\n  + [Import the Dataflow Module](#import-the-dataflow-module)\n  + [Basic Structure](#basic-structure)\n* [Regions](#regions)\n\n  + [Region Declaration](#region-declaration)\n  + [Parameterized Regions](#parameterized-regions)\n* [Kernels](#kernels)\n\n  + [Kernel Declaration](#kernel-declaration)\n  + [Single Kernel Instance](#single-kernel-instance)\n  + [Multi-Dimensional Kernel Grid](#multi-dimensional-kernel-grid)\n  + [Getting Kernel Position](#getting-kernel-position)\n* [Streams](#streams)\n\n  + [Stream Declaration](#stream-declaration)\n  + [Stream Arrays](#stream-arrays)\n  + [Stream of Blocks](#stream-of-blocks)\n  + [Stream Operations](#stream-operations)\n* [Systolic Array Patterns](#systolic-array-patterns)\n\n  + [Basic Systolic Structure](#basic-systolic-structure)\n* [Tiled Computation](#tiled-computation)\n* [Hierarchical Regions](#hierarchical-regions)\n* [Building and Execution](#building-and-execution)\n\n  + [Build for Simulation](#build-for-simulation)\n  + [Build for LLVM](#build-for-llvm)\n  + [Build for HLS](#build-for-hls)\n  + [Customization (Schedule Primitives)](#customization-schedule-primitives)\n* [Complete Example: Producer-Consumer](#complete-example-producer-consumer)\n* [Tensor Layouts](#tensor-layouts)\n\n  + [Layout Concepts](#layout-concepts)\n  + [Import and Setup](#import-and-setup)\n  + [Applying Layouts to Kernel Arguments](#applying-layouts-to-kernel-arguments)\n  + [Layout Patterns](#layout-patterns)\n  + [Understanding Mesh Axis Mapping](#understanding-mesh-axis-mapping)\n  + [How Layouts Compute Local Shapes](#how-layouts-compute-local-shapes)\n* [Best Practices](#best-practices)\n\n[Overview](#id1)[\u00b6](#overview \"Link to this heading\")\n-----------------------------------------------------\n\nModern deep learning workloads are increasingly **memory-bound**, with many kernels\nstalling on data movement rather than computation. While dataflow accelerators\n(FPGAs, NPUs) offer on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models fall into two extremes:\n\n* **Low-level interfaces** provide fine-grained control but impose significant\n  development overhead and poor portability\n* **High-level tile-based languages** abstract away communication, which restricts\n  optimization of complex dataflow patterns and forces compilers to reconstruct\n  the intended dataflow\n\nThe Allo dataflow programming model provides a **middle ground** by elevating\ndata communication and sharding to **first-class constructs**. Developers express\nprograms as a graph of **tasks (kernels)** connected via explicit **stream types**,\nenabling efficient on-chip data reuse without spilling back to DRAM.\n\n### [Key Concepts](#id2)[\u00b6](#key-concepts \"Link to this heading\")\n\nThe dataflow model in Allo consists of three core abstractions:\n\n* **Regions**: Define the dataflow graph with inputs, outputs, and streams\n* **Kernels**: Units of computation mapped onto processing elements (PEs)\n* **Streams**: FIFO-based communication channels between kernels\n\n[Getting Started](#id3)[\u00b6](#getting-started \"Link to this heading\")\n-------------------------------------------------------------------\n\n### [Import the Dataflow Module](#id4)[\u00b6](#import-the-dataflow-module \"Link to this heading\")\n\n```\nimport allo\nfrom allo.ir.types import float32, Stream\nimport allo.dataflow as df\n\n# For AIE targets with tensor distribution:\nfrom allo.memory import Layout\nS = Layout.Shard       # Shorthand for sharding\nR = Layout.Replicate   # Shorthand for replication\n```\n\n### [Basic Structure](#id5)[\u00b6](#basic-structure \"Link to this heading\")\n\nA dataflow program consists of:\n\n1. A **region** decorated with `@df.region()` that defines inputs/outputs\n2. One or more **kernels** decorated with `@df.kernel()` inside the region\n3. **Streams** declared in the region for inter-kernel communication\n\n```\n@df.region()\ndef top(A: float32[M, N], B: float32[M, N]):\n    pipe: Stream[float32, 4]  # Stream with depth 4\n\n    @df.kernel(mapping=[1], args=[A])\n    def producer(local_A: float32[M, N]):\n        # Kernel logic\n        pass\n\n    @df.kernel(mapping=[1], args=[B])\n    def consumer(local_B: float32[M, N]):\n        # Kernel logic\n        pass\n```\n\n[Regions](#id6)[\u00b6](#regions \"Link to this heading\")\n---------------------------------------------------\n\nA region defines the top-level dataflow graph with its inputs, outputs, and\ninternal communication channels.\n\n### [Region Declaration](#id7)[\u00b6](#region-declaration \"Link to this heading\")\n\n```\n@df.region()\ndef top(A: float32[M, K], B: float32[K, N], C: float32[M, N]):\n    # Stream declarations\n    # Kernel definitions\n    pass\n```\n\nThe region signature specifies the external memory interfaces (inputs and outputs).\n\nImportant\n\nInside a `@df.region()`, only **stream variable declarations** and\n**``@df.kernel`` definitions** are allowed. No other function calls or\ncomputations should appear directly in the region body. All computation\nlogic must be placed inside kernels.\n\n### [Parameterized Regions](#id8)[\u00b6](#parameterized-regions \"Link to this heading\")\n\nRegions can be parameterized with type parameters for reusability:\n\n```\n@df.region()\ndef inner[P0, P1](A: float32[M, K], B: float32[K, N], C: float32[M, N]):\n    @df.kernel(mapping=[P0, P1], args=[A, B, C])\n    def gemm(local_A: float32[M, K], local_B: float32[K, N], local_C: float32[M, N]):\n        pi, pj = df.get_pid()\n        # Tiled computation using pi, pj\n        pass\n\n# Instantiate with different parallelism:\ninner[2, 2](A, B, C1)  # 2x2 grid\ninner[4, 4](A, B, C2)  # 4x4 grid\n```\n\n[Kernels](#id9)[\u00b6](#kernels \"Link to this heading\")\n---------------------------------------------------\n\nKernels are the basic units of computation in a dataflow graph.\n\n### [Kernel Declaration](#id10)[\u00b6](#kernel-declaration \"Link to this heading\")\n\n```\n@df.kernel(mapping=[P0, P1], args=[A, B, C])\ndef kernel_name(local_A: Ty[...], local_B: Ty[...], local_C: Ty[...]):\n    # Kernel body\n    pass\n```\n\n**Parameters:**\n\n* `mapping`: A list specifying the kernel\u2019s parallel instantiation shape\n* `args`: A list of region-level arguments accessible by this kernel.\n  The element at index `i` in the `args` list is passed as the `i`-th argument to the kernel function (so the number and argument type must match).\n  `args` is optional and can be omitted if the kernel takes no arguments.\n\n### [Single Kernel Instance](#id11)[\u00b6](#single-kernel-instance \"Link to this heading\")\n\nFor a single kernel instance, use `mapping=[1]`:\n\n```\n@df.kernel(mapping=[1], args=[A])\ndef producer(local_A: float32[M, N]):\n    for i, j in allo.grid(M, N):\n        pipe.put(local_A[i, j])\n```\n\n### [Multi-Dimensional Kernel Grid](#id12)[\u00b6](#multi-dimensional-kernel-grid \"Link to this heading\")\n\nFor multi-kernel arrays (e.g., systolic arrays), specify multiple dimensions:\n\n```\n@df.kernel(mapping=[P0, P1], args=[A, B, C])\ndef gemm(local_A: float32[M, K], local_B: float32[K, N], local_C: float32[M, N]):\n    pi, pj = df.get_pid()  # Get kernel's position in the grid\n    # Kernel executes at position (pi, pj)\n    pass\n```\n\n### [Getting Kernel Position](#id13)[\u00b6](#getting-kernel-position \"Link to this heading\")\n\nUse `df.get_pid()` to retrieve the kernel\u2019s position in the mapping grid, which returns a per-kernel compile-time constant variable:\n\n```\n# For 1D mapping\ni = df.get_pid()\n\n# For 2D mapping\npi, pj = df.get_pid()\n\n# For 3D mapping\npi, pj, pk = df.get_pid()\n```\n\n[Streams](#id14)[\u00b6](#streams \"Link to this heading\")\n----------------------------------------------------\n\nStreams provide FIFO-based communication between kernels.\n\n### [Stream Declaration](#id15)[\u00b6](#stream-declaration \"Link to this heading\")\n\nDeclare streams inside a region with their element type and buffer depth:\n\n```\n# Basic scalar stream\npipe: Stream[float32, 4]  # Stream of float32 with depth 4\n\n# Stream of unsigned integers\nstream: Stream[UInt(16), 4]\n```\n\n### [Stream Arrays](#id16)[\u00b6](#stream-arrays \"Link to this heading\")\n\nFor multi-kernel designs, declare arrays of streams:\n\n```\n# 2D array of streams for systolic array communication\nfifo_A: Stream[float32, 4][P0, P1]\nfifo_B: Stream[float32, 4][P0, P1]\n```\n\n### [Stream of Blocks](#id17)[\u00b6](#stream-of-blocks \"Link to this heading\")\n\nStreams can carry tensor blocks (for coarse-grained data movement):\n\n```\n# Stream where each element is a 4x4 block\npipe: Stream[int16[M, N], 4]\n\n# Stream of 1D blocks\npipe: Stream[float32[M], 4]\n\n# Array of block streams\npipe: Stream[float32[Mt, Nt], 2][P0, P1]\n```\n\n### [Stream Operations](#id18)[\u00b6](#stream-operations \"Link to this heading\")\n\n**Put (Send):**\n\n```\n# Send a scalar value\npipe.put(value)\n\n# Send to a specific stream in an array\nfifo_A[i, j + 1].put(a)\n\n# Send a block\nblock: float32[M, N] = 0\nfor m, n in allo.grid(M, N):\n    block[m, n] = local_A[i * M + m, n]\npipe.put(block)\n```\n\n**Get (Receive):**\n\n```\n# Receive a scalar value\ndata = pipe.get()\n\n# Receive from a specific stream in an array\na: float32 = fifo_A[i, j].get()\n\n# Receive a block\nblock: float32[M, N] = pipe.get()\n```\n\n[Systolic Array Patterns](#id19)[\u00b6](#systolic-array-patterns \"Link to this heading\")\n------------------------------------------------------------------------------------\n\nSystolic arrays are a common design pattern in dataflow programming.\n\n### [Basic Systolic Structure](#id20)[\u00b6](#basic-systolic-structure \"Link to this heading\")\n\nA typical systolic array has:\n\n* **Loader kernels**: Feed data from external memory to the edge of the array\n* **Compute kernels**: Perform the main computation while passing data to neighbors\n* **Drain kernels**: Remove data that has passed through the array\n\n```\n@df.region()\ndef top(A: float32[M, K], B: float32[K, N], C: float32[M, N]):\n    fifo_A: Stream[float32, 4][P0, P1]\n    fifo_B: Stream[float32, 4][P0, P1]\n\n    @df.kernel(mapping=[P0, P1], args=[A, B, C])\n    def gemm(local_A: float32[M, K], local_B: float32[K, N], local_C: float32[M, N]):\n        i, j = df.get_pid()\n\n        # Corner: skip\n        with allo.meta_if(i in {0, M + 1} and j in {0, N + 1}):\n            pass\n\n        # Left edge: load A\n        with allo.meta_elif(j == 0):\n            for k in range(K):\n                fifo_A[i, j + 1].put(local_A[i - 1, k])\n\n        # Top edge: load B\n        with allo.meta_elif(i == 0):\n            for k in range(K):\n                fifo_B[i + 1, j].put(local_B[k, j - 1])\n\n        # Right edge: drain A\n        with allo.meta_elif(j == N + 1 and i > 0):\n            for k in range(K):\n                a: float32 = fifo_A[i, j].get()\n\n        # Bottom edge: drain B\n        with allo.meta_elif(i == M + 1 and j > 0):\n            for k in range(K):\n                b: float32 = fifo_B[i, j].get()\n\n        # Interior: compute and forward\n        with allo.meta_else():\n            c: float32 = 0\n            for k in range(K):\n                a: float32 = fifo_A[i, j].get()\n                b: float32 = fifo_B[i, j].get()\n                c += a * b\n                fifo_A[i, j + 1].put(a)  # Forward to right neighbor\n                fifo_B[i + 1, j].put(b)  # Forward to bottom neighbor\n            local_C[i - 1, j - 1] = c\n```\n\n[Tiled Computation](#id21)[\u00b6](#tiled-computation \"Link to this heading\")\n------------------------------------------------------------------------\n\nFor large computations, tiles the work across multiple kernels:\n\n```\nM, N, K = 32, 32, 32\nP0, P1 = 2, 2\nMt, Nt = M // P0, N // P1\n\n@df.region()\ndef top(A: float32[M, K], B: float32[K, N], C: float32[M, N]):\n    @df.kernel(mapping=[P0, P1], args=[A, B, C])\n    def gemm(local_A: float32[M, K], local_B: float32[K, N], local_C: float32[M, N]):\n        pi, pj = df.get_pid()\n        # Each kernel computes its tile\n        for i in range(pi * Mt, (pi + 1) * Mt):\n            for j in range(pj * Nt, (pj + 1) * Nt):\n                for k in range(K):\n                    local_C[i, j] += local_A[i, k] * local_B[k, j]\n```\n\n[Hierarchical Regions](#id22)[\u00b6](#hierarchical-regions \"Link to this heading\")\n------------------------------------------------------------------------------\n\nRegions can call other regions for hierarchical decomposition:\n\n```\n@df.region()\ndef inner[P0, P1](A: float32[M, K], B: float32[K, N], C: float32[M, N]):\n    @df.kernel(mapping=[P0, P1], args=[A, B, C])\n    def gemm(local_A: float32[M, K], local_B: float32[K, N], local_C: float32[M, N]):\n        pi, pj = df.get_pid()\n        Mt: ConstExpr[int32] = M // P0\n        Nt: ConstExpr[int32] = N // P1\n        for i in range(pi * Mt, (pi + 1) * Mt):\n            for j in range(pj * Nt, (pj + 1) * Nt):\n                for k in range(K):\n                    local_C[i, j] += local_A[i, k] * local_B[k, j]\n\n@df.region()\ndef top(A: float32[M, K], B: float32[K, N], C1: float32[M, N], C2: float32[M, N]):\n    @df.kernel(mapping=[2], args=[A, B, C1, C2])\n    def wrapper(local_A: float32[M, K], local_B: float32[K, N],\n               local_C1: float32[M, N], local_C2: float32[M, N]):\n        i = df.get_pid()\n        with allo.meta_if(i == 0):\n            inner[2, 2](local_A, local_B, local_C1)  # 2x2 grid\n        with allo.meta_if(i == 1):\n            inner[4, 4](local_A, local_B, local_C2)  # 4x4 grid\n```\n\n[Building and Execution](#id23)[\u00b6](#building-and-execution \"Link to this heading\")\n----------------------------------------------------------------------------------\n\n### [Build for Simulation](#id24)[\u00b6](#build-for-simulation \"Link to this heading\")\n\nUse the simulator target for functional verification:\n\n```\nsim_mod = df.build(top, target=\"simulator\")\nsim_mod(A, B, C)\nnp.testing.assert_allclose(C, np.dot(A, B), atol=1e-5)\n```\n\n### [Build for LLVM](#id25)[\u00b6](#build-for-llvm \"Link to this heading\")\n\nBuild for CPU execution (mainly for debugging):\n\n```\nmod = df.build(top)  # Default LLVM target\nmod(A, B, C)\n```\n\n### [Build for HLS](#id26)[\u00b6](#build-for-hls \"Link to this heading\")\n\nGenerate Vitis HLS code:\n\n```\n# Generate HLS code only\nmod = df.build(top, target=\"vhls\")\nprint(mod.hls_code)\n\n# Build and run HLS C-simulation\nmod = df.build(top, target=\"vitis_hls\", mode=\"csim\", project=\"myproject.prj\")\nmod(A, B, C)\n\n# Build for hardware emulation\nmod = df.build(top, target=\"vitis_hls\", mode=\"hw_emu\", project=\"myproject.prj\")\nmod(A, B, C)\n```\n\n### [Customization (Schedule Primitives)](#id27)[\u00b6](#customization-schedule-primitives \"Link to this heading\")\n\nApply schedule primitives before building:\n\n```\ns = df.customize(top)\ns.partition(\"top:A\", dim=1, factor=2)\ns.partition(\"top:B\", dim=2, factor=2)\ns.partition(\"top:C\", dim=0, factor=2)\nmod = s.build(target=\"vitis_hls\", mode=\"hw_emu\", project=\"myproject.prj\")\n```\n\n[Complete Example: Producer-Consumer](#id28)[\u00b6](#complete-example-producer-consumer \"Link to this heading\")\n-----------------------------------------------------------------------------------------------------------\n\nA complete example showing a simple producer-consumer pattern:\n\n```\nimport allo\nfrom allo.ir.types import float32, Stream\nimport allo.dataflow as df\nimport numpy as np\n\nM, N = 16, 16\n\n@df.region()\ndef top(A: float32[M, N], B: float32[M, N]):\n    pipe: Stream[float32, 4]\n\n    @df.kernel(mapping=[1], args=[A])\n    def producer(local_A: float32[M, N]):\n        for i, j in allo.grid(M, N):\n            out: float32 = local_A[i, j]\n            pipe.put(out)\n\n    @df.kernel(mapping=[1], args=[B])\n    def consumer(local_B: float32[M, N]):\n        for i, j in allo.grid(M, N):\n            data = pipe.get()\n            local_B[i, j] = data + 1\n\n# Run with simulator\nA = np.random.rand(M, N).astype(np.float32)\nB = np.zeros((M, N), dtype=np.float32)\nsim_mod = df.build(top, target=\"simulator\")\nsim_mod(A, B)\nnp.testing.assert_allclose(B, A + 1)\nprint(\"Passed!\")\n```\n\n[Tensor Layouts](#id29)[\u00b6](#tensor-layouts \"Link to this heading\")\n------------------------------------------------------------------\n\nWhen targeting hardware accelerators like AIE (AI Engine), tensors need to be\ndistributed across multiple processing elements (PEs). The `Layout` class\nprovides a declarative way to specify how global tensors are partitioned and\nmapped to local PE memories.\n\n### [Layout Concepts](#id30)[\u00b6](#layout-concepts \"Link to this heading\")\n\nThe `Layout` class encodes a **partitioning scheme** for a tensor. For each\n*tensor dimension*, you specify either:\n\n* **Shard(axis)**: Partition this dimension across the specified *mesh axis*\n* **Replicate**: Keep this dimension fully replicated across all PEs\n\n### [Import and Setup](#id31)[\u00b6](#import-and-setup \"Link to this heading\")\n\n```\nfrom allo.memory import Layout\n\nS = Layout.Shard   # Shorthand for sharding\nR = Layout.Replicate  # Shorthand for replication\n```\n\n### [Applying Layouts to Kernel Arguments](#id32)[\u00b6](#applying-layouts-to-kernel-arguments \"Link to this heading\")\n\nUse the `@` operator to annotate kernel arguments with their layout. If no layout is specified, the default is **replicated** for all dimensions:\n\n```\nLyA = [S(0), R]  # Shard first dim on mesh axis 0, replicate second dim\n\n@df.kernel(mapping=[P0, P1], args=[A, B])\ndef kernel(local_A: Ty[M, N] @ LyA, local_B: Ty[M, N]):\n    # local_A is automatically partitioned according to LyA\n    # local_B has no layout annotation (defaults to Replicate)\n    pass\n```\n\n### [Layout Patterns](#id33)[\u00b6](#layout-patterns \"Link to this heading\")\n\n**1D Tensor Sharding:**\n\n```\n# Vector of size M, sharded across 4 PEs\nLy = [S(0)]  # Shard on mesh axis 0\n\n@df.kernel(mapping=[4], args=[A, B])\ndef core(local_A: int32[M] @ Ly, local_B: int32[M] @ Ly):\n    # Each PE gets M/4 elements\n    local_B[:] = allo.add(local_A, 1)\n```\n\n**2D Tensor Row Sharding:**\n\n```\n# Matrix [M, N], shard rows across mesh axis 0\nLyA = [S(0), R]\n\n@df.kernel(mapping=[4], args=[A])\ndef kernel(local_A: Ty[M, N] @ LyA):\n    # Each PE gets M/4 rows (full columns)\n    pass\n```\n\n**2D Tensor Column Sharding:**\n\n```\n# Matrix [M, N], shard columns across mesh axis 0\nLyB = [R, S(0)]\n\n@df.kernel(mapping=[4], args=[B])\ndef kernel(local_B: Ty[M, N] @ LyB):\n    # Each PE gets full rows, N/4 columns\n    pass\n```\n\n**2D Tensor Full Sharding (GEMM Example):**\n\nFor matrix multiplication `C = A @ B` with 2D PE grid:\n\n```\nM, N, K = 64, 64, 64\nP0, P1 = 2, 2  # 2x2 PE grid\n\n# A[M, K]: shard M on axis 1 (for row parallelism), replicate K\nLyA = [S(1), R]\n# B[K, N]: replicate K, shard N on axis 0 (for column parallelism)\nLyB = [R, S(0)]\n# C[M, N]: shard both dimensions\nLyC = [S(1), S(0)]\n\n@df.region()\ndef top(A: Ty[M, K], B: Ty[K, N], C: Ty[M, N]):\n    @df.kernel(mapping=[P0, P1], args=[A, B, C])\n    def gemm(\n        local_A: Ty[M, K] @ LyA,\n        local_B: Ty[K, N] @ LyB,\n        local_C: Ty[M, N] @ LyC\n    ):\n        # Each PE computes a tile of C\n        local_C[:, :] = allo.matmul(local_A, local_B)\n```\n\n**3D Sharding for Temporal Reduction (Split-K GEMM):**\n\n```\nM, N, K = 64, 64, 128\nPk, Pm, Pn = 4, 2, 2  # 4x2x2 PE grid\n\n# A[M, K]: shard M on axis 1, shard K on axis 0\nLyA = [S(1), S(0)]\n# B[K, N]: shard K on axis 0, shard N on axis 2\nLyB = [S(0), S(2)]\n# C[M, N]: shard M on axis 1, shard N on axis 2\nLyC = [S(1), S(2)]\n\n@df.kernel(mapping=[Pk, Pm, Pn], args=[A, B, C])\ndef gemm(\n    local_A: Ty[M, K] @ LyA,\n    local_B: Ty[K, N] @ LyB,\n    local_C: Ty[M, N] @ LyC\n):\n    pk, pm, pn = df.get_pid()\n    # Each PE along the K dimension computes a partial sum\n```\n\n### [Understanding Mesh Axis Mapping](#id34)[\u00b6](#understanding-mesh-axis-mapping \"Link to this heading\")\n\nThe `Shard(axis)` parameter refers to the **mesh dimension** (from the kernel\u2019s\n`mapping` parameter), not the tensor dimension:\n\n```\nKernel mapping: [P0, P1, P2] -> 3D mesh with axes 0, 1, 2\n                 |   |   |\n                 0   1   2  <- mesh axes\n\nLayout [S(1), R] means:\n  - Tensor dim 0: sharded across mesh axis 1 (P1 partitions)\n  - Tensor dim 1: replicated (each PE has full dim)\n```\n\n### [How Layouts Compute Local Shapes](#id35)[\u00b6](#how-layouts-compute-local-shapes \"Link to this heading\")\n\nThe local tensor shape at each PE is automatically computed:\n\n```\n# Global tensor: [64, 64]\n# Layout: [S(0), S(1)]\n# Mapping: [4, 2]  -> 4x2 PE grid\n#\n# Local shape per PE: [64/4, 64/2] = [16, 32]\n```\n\nFor replicated dimensions, the local shape equals the global shape for that dimension.\n\n[Best Practices](#id36)[\u00b6](#best-practices \"Link to this heading\")\n------------------------------------------------------------------\n\n1. **Start with the simulator**: Use `target=\"simulator\"` for initial debugging\n   before moving to HLS.\n2. **Match stream types carefully**: Ensure `put` and `get` operations match\n   the declared stream element type.\n3. **Use meta conditionals for PE differentiation**: In systolic arrays, use\n   `allo.meta_if/meta_elif/meta_else` to generate different code for different\n   PE positions.\n4. **Consider stream depth**: Set appropriate FIFO depths based on the\n   producer-consumer latency mismatch.\n5. **Use ConstExpr for tile sizes**: When computing tile boundaries, use\n   `ConstExpr` for values that should be computed at compile time.\n\n[<Page contents](#)\n\n[>Page contents:](#)\n\n* [Dataflow Programming](#)\n  + [Overview](#overview)\n    - [Key Concepts](#key-concepts)\n  + [Getting Started](#getting-started)\n    - [Import the Dataflow Module](#import-the-dataflow-module)\n    - [Basic Structure](#basic-structure)\n  + [Regions](#regions)\n    - [Region Declaration](#region-declaration)\n    - [Parameterized Regions](#parameterized-regions)\n  + [Kernels](#kernels)\n    - [Kernel Declaration](#kernel-declaration)\n    - [Single Kernel Instance](#single-kernel-instance)\n    - [Multi-Dimensional Kernel Grid](#multi-dimensional-kernel-grid)\n    - [Getting Kernel Position](#getting-kernel-position)\n  + [Streams](#streams)\n    - [Stream Declaration](#stream-declaration)\n    - [Stream Arrays](#stream-arrays)\n    - [Stream of Blocks](#stream-of-blocks)\n    - [Stream Operations](#stream-operations)\n  + [Systolic Array Patterns](#systolic-array-patterns)\n    - [Basic Systolic Structure](#basic-systolic-structure)\n  + [Tiled Computation](#tiled-computation)\n  + [Hierarchical Regions](#hierarchical-regions)\n  + [Building and Execution](#building-and-execution)\n    - [Build for Simulation](#build-for-simulation)\n    - [Build for LLVM](#build-for-llvm)\n    - [Build for HLS](#build-for-hls)\n    - [Customization (Schedule Primitives)](#customization-schedule-primitives)\n  + [Complete Example: Producer-Consumer](#complete-example-producer-consumer)\n  + [Tensor Layouts](#tensor-layouts)\n    - [Layout Concepts](#layout-concepts)\n    - [Import and Setup](#import-and-setup)\n    - [Applying Layouts to Kernel Arguments](#applying-layouts-to-kernel-arguments)\n    - [Layout Patterns](#layout-patterns)\n    - [Understanding Mesh Axis Mapping](#understanding-mesh-axis-mapping)\n    - [How Layouts Compute Local Shapes](#how-layouts-compute-local-shapes)\n  + [Best Practices](#best-practices)\n\n[<Frontend Syntax Guide](frontend_syntax.html)\n\n[Data Types and Type Casting>](../gallery/dive_01_data_types.html)\n\n\u00a9 Copyright 2025, Allo Authors.\nCreated using [Sphinx](https://www.sphinx-doc.org/) 8.2.3.\n\nStyled using the [Piccolo Theme](https://github.com/piccolo-orm/piccolo_theme)"
  }
]
